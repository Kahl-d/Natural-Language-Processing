{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSC 820 NLT\n",
    "## HW11\n",
    "* > Khalid Mehtab Khan\n",
    "* > 923673423\n",
    "\n",
    "## Program_0    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015843,
     "end_time": "2021-03-10T09:03:36.533752",
     "exception": false,
     "start_time": "2021-03-10T09:03:36.517909",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Text Mining : TF-IDF and Cosine Similarity from Scratch\n",
    "\n",
    "Table of Contents:\n",
    "\n",
    "1. Term Frequency (TF)\n",
    "2. Inverse Document Frequency (IDF)\n",
    "3. TF * IDF\n",
    "4. Vector Space Models and Representation â€“ Cosine Similarity\n",
    "\n",
    "*** Any feedback or feature requests are welcome!***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014249,
     "end_time": "2021-03-10T09:03:36.562996",
     "exception": false,
     "start_time": "2021-03-10T09:03:36.548747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let us imagine that you are doing a search on below documents with the following query: **life learning**\n",
    "\n",
    "* **Document 1** : I want to start learning to charge something in life.\n",
    "* **Document 2** : learning something about me no one else knows\n",
    "* **Document 3** : Never stop learning\n",
    "\n",
    "The query is a free text query. It means a query in which the terms of the query are typed freeform into the search interface, without any connecting search operators.\n",
    "\n",
    "Let us go over each step in detail to see how it all works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015949,
     "end_time": "2021-03-10T09:03:36.593749",
     "exception": false,
     "start_time": "2021-03-10T09:03:36.577800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1.Term Frequency(TF)\n",
    "Term Frequency also known as TF measures the number of times a term (word) occurs in a document. Given below is the code and the terms and their frequency on each of the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprting required libraries\n",
    "# math for mathematical operations\n",
    "# numpy and pandas for data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T09:03:36.628257Z",
     "iopub.status.busy": "2021-03-10T09:03:36.627574Z",
     "iopub.status.idle": "2021-03-10T09:03:36.630522Z",
     "shell.execute_reply": "2021-03-10T09:03:36.629964Z"
    },
    "papermill": {
     "duration": 0.022235,
     "end_time": "2021-03-10T09:03:36.630683",
     "exception": false,
     "start_time": "2021-03-10T09:03:36.608448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Toy dataset\n",
    "# 3 documents\n",
    "# 1 query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents\n",
    "doc1 = \"I want to start learning to charge something in life\"\n",
    "doc2 = \"reading something about life no one else knows\"\n",
    "doc3 = \"Never stop learning\"\n",
    "#query string\n",
    "query = \"life learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents are stored in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I want to start learning to charge something in life',\n",
       " 'reading something about life no one else knows',\n",
       " 'Never stop learning']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = [doc1, doc2, doc3]\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01451,
     "end_time": "2021-03-10T09:03:36.699155",
     "exception": false,
     "start_time": "2021-03-10T09:03:36.684645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**NOTE :** Text Preprocessing Steps are ignored as the objective of this kernel is to explain and develop TF-IDF and cosine similarity from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute-tf function\n",
    "\n",
    "# This function computes the frequency of each word in the document\n",
    "# For each document in the list of documents,\n",
    "# we split the document into words and count the frequency of each word\n",
    "# We store the frequency of each word in a temperory dictionary\n",
    "# Word frequencies for each document is printed at the end of each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T09:03:36.739146Z",
     "iopub.status.busy": "2021-03-10T09:03:36.738408Z",
     "iopub.status.idle": "2021-03-10T09:03:36.769276Z",
     "shell.execute_reply": "2021-03-10T09:03:36.769779Z"
    },
    "papermill": {
     "duration": 0.055228,
     "end_time": "2021-03-10T09:03:36.769940",
     "exception": false,
     "start_time": "2021-03-10T09:03:36.714712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Document  to  want  start  I  in  something  learning  life  charge\n",
      "0  Term Frequency   2     1      1  1   1          1         1     1       1\n",
      "         Document  knows  no  one  about  something  reading  else  life\n",
      "0  Term Frequency      1   1    1      1          1        1     1     1\n",
      "         Document  stop  Never  learning\n",
      "0  Term Frequency     1      1         1\n"
     ]
    }
   ],
   "source": [
    "#term -frequenvy :word occurences in a document\n",
    "def compute_tf(docs_list):\n",
    "    for doc in docs_list:\n",
    "        doc1_lst = doc.split(\" \")\n",
    "        wordDict_1= dict.fromkeys(set(doc1_lst), 0)\n",
    "\n",
    "        for token in doc1_lst:\n",
    "            wordDict_1[token] +=  1\n",
    "        df = pd.DataFrame([wordDict_1])\n",
    "        idx = 0\n",
    "        new_col = [\"Term Frequency\"]    \n",
    "        df.insert(loc=idx, column='Document', value=new_col)\n",
    "        print(df)\n",
    "        \n",
    "compute_tf(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see documents are like a data frame now\n",
    "\n",
    "# Sentences are toekinzed into words\n",
    "# for each token we have a frequency count in that document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015216,
     "end_time": "2021-03-10T09:03:36.801004",
     "exception": false,
     "start_time": "2021-03-10T09:03:36.785788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* In reality each document will be of different size. On a large document the frequency of the terms will be much higher than the smaller ones. Hence we need to normalize the document based on its size. \n",
    "* A simple trick is to divide the term frequency by the total number of terms. \n",
    "* For example in Document 1 the term game occurs two times. The total number of terms in the document is 10. Hence the normalized term frequency is 2 / 10 = 0.2. \n",
    "\n",
    "\n",
    "Given below are the normalized term frequency for all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_normalizedtf function\n",
    "\n",
    "# This function calculates the normalized term frequency for each word in a document.\n",
    "# It first breaks down each document into words and converts them to lowercase.\n",
    "# Each word's frequency in the document is divided by the total number of words to get its normalized frequency.\n",
    "# This frequency is stored in a dictionary. tf_doc\n",
    "\n",
    "# It also displays a DataFrame for each document, showing these frequencies under the column \"Normalized TF\".\n",
    "# The function returns a list of these dictionaries, one for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T09:03:36.854003Z",
     "iopub.status.busy": "2021-03-10T09:03:36.848780Z",
     "iopub.status.idle": "2021-03-10T09:03:36.862977Z",
     "shell.execute_reply": "2021-03-10T09:03:36.862198Z"
    },
    "papermill": {
     "duration": 0.046729,
     "end_time": "2021-03-10T09:03:36.863140",
     "exception": false,
     "start_time": "2021-03-10T09:03:36.816411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Document   to  want  start    I   in  something  learning  life  \\\n",
      "0  Normalized TF  0.2   0.1    0.1  0.1  0.1        0.1       0.1   0.1   \n",
      "\n",
      "   charge  \n",
      "0     0.1  \n",
      "        Document  knows     no    one  about  something  reading   else   life\n",
      "0  Normalized TF  0.125  0.125  0.125  0.125      0.125    0.125  0.125  0.125\n",
      "        Document      stop     Never  learning\n",
      "0  Normalized TF  0.333333  0.333333  0.333333\n"
     ]
    }
   ],
   "source": [
    "#Normalized Term Frequency\n",
    "def termFrequency(term, document):\n",
    "    normalizeDocument = document.lower().split()\n",
    "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n",
    "\n",
    "def compute_normalizedtf(documents):\n",
    "    tf_doc = []\n",
    "    for txt in documents:\n",
    "        sentence = txt.split()\n",
    "        norm_tf= dict.fromkeys(set(sentence), 0)\n",
    "        for word in sentence:\n",
    "            norm_tf[word] = termFrequency(word, txt)\n",
    "        tf_doc.append(norm_tf)\n",
    "        df = pd.DataFrame([norm_tf])\n",
    "        idx = 0\n",
    "        new_col = [\"Normalized TF\"]    \n",
    "        df.insert(loc=idx, column='Document', value=new_col)\n",
    "        print(df)\n",
    "    return tf_doc\n",
    "\n",
    "tf_doc = compute_normalizedtf(documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is the normalized word frequency of every word in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'to': 0.2,\n",
       "  'want': 0.1,\n",
       "  'start': 0.1,\n",
       "  'I': 0.1,\n",
       "  'in': 0.1,\n",
       "  'something': 0.1,\n",
       "  'learning': 0.1,\n",
       "  'life': 0.1,\n",
       "  'charge': 0.1},\n",
       " {'knows': 0.125,\n",
       "  'no': 0.125,\n",
       "  'one': 0.125,\n",
       "  'about': 0.125,\n",
       "  'something': 0.125,\n",
       "  'reading': 0.125,\n",
       "  'else': 0.125,\n",
       "  'life': 0.125},\n",
       " {'stop': 0.3333333333333333,\n",
       "  'Never': 0.3333333333333333,\n",
       "  'learning': 0.3333333333333333}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the first document, \n",
    "# words like \"life,\" \"I,\" \"start,\" and others appear once in the document, so they have the same normalized frequency of 0.1. AS there are 10 words.\n",
    "# In the third document, the words \"Never,\" \"stop,\" and \"learning\" all have an equal frequency of 0.333333, \n",
    "# since there are only three words in this document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016057,
     "end_time": "2021-03-10T09:03:36.895610",
     "exception": false,
     "start_time": "2021-03-10T09:03:36.879553",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Inverse Document Frequency (IDF)\n",
    "\n",
    "* The main purpose of doing a search is to find out relevant documents matching the query. \n",
    "* In Term Frequecy all terms are considered equally important. In fact certain terms that occur too frequently have little power in determining the relevance. \n",
    "* We need a way to weigh down the effects of too frequently occurring terms. Also the terms that occur less in the document can be more relevant. \n",
    "* We need a way to weigh up the effects of less frequently occurring terms. Logarithms helps us to solve this problem.Logarithms helps us to solve this problem.\n",
    "\n",
    "\n",
    "Let us compute IDF for the term start\n",
    "\n",
    "IDF(start) = 1 + loge(Total Number Of Documents / Number Of Documents with term start in it)\n",
    "\n",
    "There are 3 documents in all = Document1, Document2, Document3\n",
    "The term start appears in Document1\n",
    "\n",
    " IDF(start) = 1 + loge(3 / 1)\n",
    "            = 1 + 1.098726209\n",
    "            = 2.098726209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_idf function\n",
    "\n",
    "# This function figures out how unique a word is across all documents.\n",
    "# It checks how many documents contain each word.\n",
    "# The more documents a word is in, the less unique it is.\n",
    "# We do some math to find a score (IDF) that gets bigger the rarer the word is.\n",
    "# If a word isn't in any documents, we just give it a score of 1.\n",
    "# At the end, it gives us a list with all the words and their uniqueness scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T09:03:36.942741Z",
     "iopub.status.busy": "2021-03-10T09:03:36.942030Z",
     "iopub.status.idle": "2021-03-10T09:03:36.946998Z",
     "shell.execute_reply": "2021-03-10T09:03:36.946265Z"
    },
    "papermill": {
     "duration": 0.03524,
     "end_time": "2021-03-10T09:03:36.947120",
     "exception": false,
     "start_time": "2021-03-10T09:03:36.911880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I': 2.09861228866811,\n",
       " 'want': 2.09861228866811,\n",
       " 'to': 2.09861228866811,\n",
       " 'start': 2.09861228866811,\n",
       " 'learning': 1.4054651081081644,\n",
       " 'charge': 2.09861228866811,\n",
       " 'something': 1.4054651081081644,\n",
       " 'in': 2.09861228866811,\n",
       " 'life': 1.4054651081081644,\n",
       " 'reading': 2.09861228866811,\n",
       " 'about': 2.09861228866811,\n",
       " 'no': 2.09861228866811,\n",
       " 'one': 2.09861228866811,\n",
       " 'else': 2.09861228866811,\n",
       " 'knows': 2.09861228866811,\n",
       " 'Never': 2.09861228866811,\n",
       " 'stop': 2.09861228866811}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inverseDocumentFrequency(term, allDocuments):\n",
    "    numDocumentsWithThisTerm = 0\n",
    "    for doc in range (0, len(allDocuments)):\n",
    "        if term.lower() in allDocuments[doc].lower().split():\n",
    "            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1\n",
    " \n",
    "    if numDocumentsWithThisTerm > 0:\n",
    "        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "def compute_idf(documents):\n",
    "    idf_dict = {}\n",
    "    for doc in documents:\n",
    "        sentence = doc.split()\n",
    "        for word in sentence:\n",
    "            idf_dict[word] = inverseDocumentFrequency(word, documents)\n",
    "    return idf_dict\n",
    "idf_dict = compute_idf(documents)\n",
    "\n",
    "compute_idf(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The numbers next to each word are the IDF scores, which tell us how common or rare a word is across all the documents you've used. \n",
    "# For example, the word \"learning\" has a score of about 1.4, \n",
    "# meaning it's less common across your documents than the word \"I\", which has a score of roughly 2.1. \n",
    "# The higher the score, the rarer the word. \n",
    "# These scores help in understanding which words might carry more weight in terms of uniqueness in your set of documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016677,
     "end_time": "2021-03-10T09:03:36.980514",
     "exception": false,
     "start_time": "2021-03-10T09:03:36.963837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3.TF * IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016682,
     "end_time": "2021-03-10T09:03:37.013870",
     "exception": false,
     "start_time": "2021-03-10T09:03:36.997188",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Remember we are trying to find out relevant documents for the query: **life learning**\n",
    "\n",
    "* For each term in the query multiply its normalized term frequency with its IDF on each document. \n",
    "* In Document1 for the term life the normalized term frequency is 0.1 and its IDF is 1.405465108. \n",
    "* Multiplying them together we get 0.140550715 (0.1 * 1.405465108). \n",
    "* \n",
    "Given below is TF * IDF calculations for life and learning in all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_tfidf_with_alldocs function\n",
    "\n",
    "# This function calculates the importance of each search term within each document.\n",
    "# For each document, it takes the term frequency (TF) of the search term, \n",
    "# which is how often the term appears in that particular document.\n",
    "# Then, it looks at how common or rare the term is across all documents, which is the inverse document frequency (IDF).\n",
    "# By multiplying the TF by the IDF, we get the TF-IDF score, which tells us how relevant each term is in every document.\n",
    "# It does this for each term in the search query and for each document.\n",
    "# Finally, it provides us with a score for the search query for each document, \n",
    "# helping to identify the most relevant documents for the given search terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T09:03:37.070339Z",
     "iopub.status.busy": "2021-03-10T09:03:37.069470Z",
     "iopub.status.idle": "2021-03-10T09:03:37.073407Z",
     "shell.execute_reply": "2021-03-10T09:03:37.072766Z"
    },
    "papermill": {
     "duration": 0.043109,
     "end_time": "2021-03-10T09:03:37.073525",
     "exception": false,
     "start_time": "2021-03-10T09:03:37.030416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   doc      life  learning\n",
      "0  0.0  0.140547  0.140547\n",
      "1  1.0  0.175683  0.000000\n",
      "2  2.0  0.000000  0.468488\n"
     ]
    }
   ],
   "source": [
    "# tf-idf score across all docs for the query string(\"life learning\")\n",
    "def compute_tfidf_with_alldocs(documents , query):\n",
    "    tf_idf = []\n",
    "    index = 0\n",
    "    query_tokens = query.split()\n",
    "    df = pd.DataFrame(columns=['doc'] + query_tokens)\n",
    "    for doc in documents:\n",
    "        df['doc'] = np.arange(0 , len(documents))\n",
    "        doc_num = tf_doc[index]\n",
    "        sentence = doc.split()\n",
    "        for word in sentence:\n",
    "            for text in query_tokens:\n",
    "                if(text == word):\n",
    "                    idx = sentence.index(word)\n",
    "                    tf_idf_score = doc_num[word] * idf_dict[word]\n",
    "                    tf_idf.append(tf_idf_score)\n",
    "                    df.iloc[index, df.columns.get_loc(word)] = tf_idf_score\n",
    "        index += 1\n",
    "    df.fillna(0 , axis=1, inplace=True)\n",
    "    return tf_idf , df\n",
    "            \n",
    "tf_idf , df = compute_tfidf_with_alldocs(documents , query)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document 2 doesn't mention \"life\", hence the score of 0.0000, \n",
    "# but has a high relevance for \"learning\", \n",
    "# with the score being 0.4685,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017202,
     "end_time": "2021-03-10T09:03:37.107473",
     "exception": false,
     "start_time": "2021-03-10T09:03:37.090271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4.Vector Space Models and Representation  â€“ Cosine Similarity\n",
    "\n",
    "The set of documents in a collection then is viewed as a set of vectors in a vector space. Each term will have its own axis. Using the formula given below we can find out the similarity between any two documents.\n",
    "\n",
    "* > Cosine Similarity (d1, d2) =  Dot product(d1, d2) / ||d1|| * ||d2||\n",
    "* > Dot product (d1,d2) = d1[0] * d2[0] + d1[1] * d2[1] * â€¦ * d1[n] * d2[n]\n",
    "* > ||d1|| = square root(d1[0]2 + d1[1]2 + ... + d1[n]2)\n",
    "* > ||d2|| = square root(d2[0]2 + d2[1]2 + ... + d2[n]2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017628,
     "end_time": "2021-03-10T09:03:37.195478",
     "exception": false,
     "start_time": "2021-03-10T09:03:37.177850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "* Vectors deals only with numbers. In this example we are dealing with text documents. This was the reason why we used TF and IDF to convert text into numbers so that it can be represented by a vecto\n",
    "\n",
    "\n",
    "The query entered by the user can also be represented as a vector. We will calculate the TF*IDF for the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_query_tf function\n",
    "\n",
    "# This function takes a search query and finds out how often each word appears in it.\n",
    "# It splits the query into words and uses the termFrequency function to calculate each word's normalized term frequency.\n",
    "# This tells us the proportion of each word in the query, considering the length of the query.\n",
    "# The result is a dictionary where each word from the query is a key, and its normalized term frequency is the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T09:03:37.240068Z",
     "iopub.status.busy": "2021-03-10T09:03:37.239003Z",
     "iopub.status.idle": "2021-03-10T09:03:37.242904Z",
     "shell.execute_reply": "2021-03-10T09:03:37.243704Z"
    },
    "papermill": {
     "duration": 0.030684,
     "end_time": "2021-03-10T09:03:37.243916",
     "exception": false,
     "start_time": "2021-03-10T09:03:37.213232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 0.5, 'learning': 0.5}\n"
     ]
    }
   ],
   "source": [
    "#Normalized TF for the query string(\"life learning\")\n",
    "def compute_query_tf(query):\n",
    "    query_norm_tf = {}\n",
    "    tokens = query.split()\n",
    "    for word in tokens:\n",
    "        query_norm_tf[word] = termFrequency(word , query)\n",
    "    return query_norm_tf\n",
    "query_norm_tf = compute_query_tf(query)\n",
    "print(query_norm_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_query_idf function\n",
    "\n",
    "# This function works out how common or rare each word in your search query is across all the documents.\n",
    "# It splits your query into separate words and then calculates the IDF for each one.\n",
    "# It uses the inverseDocumentFrequency function we talked about before.\n",
    "# The function creates a dictionary with words from the query as keys and their IDF scores as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T09:03:37.289898Z",
     "iopub.status.busy": "2021-03-10T09:03:37.289226Z",
     "iopub.status.idle": "2021-03-10T09:03:37.292791Z",
     "shell.execute_reply": "2021-03-10T09:03:37.292120Z"
    },
    "papermill": {
     "duration": 0.028473,
     "end_time": "2021-03-10T09:03:37.292916",
     "exception": false,
     "start_time": "2021-03-10T09:03:37.264443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 1.4054651081081644, 'learning': 1.4054651081081644}\n"
     ]
    }
   ],
   "source": [
    "#idf score for the query string(\"life learning\")\n",
    "def compute_query_idf(query):\n",
    "    idf_dict_qry = {}\n",
    "    sentence = query.split()\n",
    "    \n",
    "    for word in sentence:\n",
    "        idf_dict_qry[word] = inverseDocumentFrequency(word ,documents)\n",
    "    return idf_dict_qry\n",
    "idf_dict_qry = compute_query_idf(query)\n",
    "print(idf_dict_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_query_tfidf function\n",
    "\n",
    "# This function determines the importance of each word in the search query \"life learning\" in relation to all documents.\n",
    "# It splits the query into individual words and calculates the TF-IDF score for each one.\n",
    "# It does this by multiplying the normalized term frequency (from query_norm_tf) by the inverse document frequency (from idf_dict_qry) for each word.\n",
    "# The result is a dictionary that pairs each word with its TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T09:03:37.337403Z",
     "iopub.status.busy": "2021-03-10T09:03:37.336630Z",
     "iopub.status.idle": "2021-03-10T09:03:37.340919Z",
     "shell.execute_reply": "2021-03-10T09:03:37.340248Z"
    },
    "papermill": {
     "duration": 0.029071,
     "end_time": "2021-03-10T09:03:37.341035",
     "exception": false,
     "start_time": "2021-03-10T09:03:37.311964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'life': 0.7027325540540822, 'learning': 0.7027325540540822}\n"
     ]
    }
   ],
   "source": [
    "#tf-idf score for the query string(\"life learning\")\n",
    "def compute_query_tfidf(query):\n",
    "    tfidf_dict_qry = {}\n",
    "    sentence = query.split()\n",
    "    for word in sentence:\n",
    "        tfidf_dict_qry[word] = query_norm_tf[word] * idf_dict_qry[word]\n",
    "    return tfidf_dict_qry\n",
    "tfidf_dict_qry = compute_query_tfidf(query)\n",
    "print(tfidf_dict_qry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01903,
     "end_time": "2021-03-10T09:03:37.379287",
     "exception": false,
     "start_time": "2021-03-10T09:03:37.360257",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let us now calculate the cosine similarity of the query and Document1.\n",
    "\n",
    "Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
    "\n",
    "Dot product(Query, Document1) \n",
    "     = ((0.702753576) * (0.140550715) + (0.702753576)*(0.140550715))\n",
    "     = 0.197545035151\n",
    "\n",
    "||Query|| = sqrt((0.702753576)2 + (0.702753576)2) = 0.993843638185\n",
    "\n",
    "||Document1|| = sqrt((0.140550715)2 + (0.140550715)2) = 0.198768727354\n",
    "\n",
    "Cosine Similarity(Query, Document) = 0.197545035151 / (0.993843638185) * (0.198768727354)\n",
    "                                        = 0.197545035151 / 0.197545035151\n",
    "                                        = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_similarity function\n",
    "\n",
    "# This function measures how similar a search query is to a document.\n",
    "# It takes the TF-IDF scores for the query and the document and caclucates the dot product.\n",
    "# Then it calculates the magnitude (or length) of both the query and document vectors.\n",
    "# It divides the dot product by the product of the two magnitudes to get the cosine similarity.\n",
    "# The closer this number is to 1, the more similar the document is to the search query.\n",
    "\n",
    "# There's also a helper function called 'flatten' \n",
    "# which is used to turn nested lists into a single list. \n",
    "# This can be handy when dealing with lists of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T09:03:37.430713Z",
     "iopub.status.busy": "2021-03-10T09:03:37.429853Z",
     "iopub.status.idle": "2021-03-10T09:03:37.434083Z",
     "shell.execute_reply": "2021-03-10T09:03:37.433448Z"
    },
    "papermill": {
     "duration": 0.035809,
     "end_time": "2021-03-10T09:03:37.434194",
     "exception": false,
     "start_time": "2021-03-10T09:03:37.398385",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n",
    "\n",
    "\"\"\"\n",
    "Example : Dot roduct(Query, Document1) \n",
    "\n",
    "     life:\n",
    "     = tfidf(life w.r.t query) * tfidf(life w.r.t Document1) +  / \n",
    "     sqrt(tfidf(life w.r.t query)) * \n",
    "     sqrt(tfidf(life w.r.t doc1))\n",
    "     \n",
    "     learning:\n",
    "     =tfidf(learning w.r.t query) * tfidf(learning w.r.t Document1)/\n",
    "     sqrt(tfidf(learning w.r.t query)) * \n",
    "     sqrt(tfidf(learning w.r.t doc1))\n",
    "\n",
    "\"\"\"\n",
    "def cosine_similarity(tfidf_dict_qry, df , query , doc_num):\n",
    "    dot_product = 0\n",
    "    qry_mod = 0\n",
    "    doc_mod = 0\n",
    "    tokens = query.split()\n",
    "   \n",
    "    for keyword in tokens:\n",
    "        dot_product += tfidf_dict_qry[keyword] * df[keyword][df['doc'] == doc_num]\n",
    "        #||Query||\n",
    "        qry_mod += tfidf_dict_qry[keyword] * tfidf_dict_qry[keyword]\n",
    "        #||Document||\n",
    "        doc_mod += df[keyword][df['doc'] == doc_num] * df[keyword][df['doc'] == doc_num]\n",
    "    qry_mod = np.sqrt(qry_mod)\n",
    "    doc_mod = np.sqrt(doc_mod)\n",
    "    #implement formula\n",
    "    denominator = qry_mod * doc_mod\n",
    "    cos_sim = dot_product/denominator\n",
    "     \n",
    "    return cos_sim\n",
    "\n",
    "from collections.abc import Iterable\n",
    "def flatten(lis):\n",
    "     for item in lis:\n",
    "        if isinstance(item, Iterable) and not isinstance(item, str):\n",
    "             for x in flatten(item):\n",
    "                yield x\n",
    "        else:        \n",
    "             yield item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank_similarity_docs function\n",
    "\n",
    "# This function ranks documents by how similar they are to the search query.\n",
    "# It goes through each document and uses the cosine_similarity function to calculate similarity.\n",
    "# Each similarity score is added to a list.\n",
    "# The function returns this list of cosine similarity scores, one for each document.\n",
    "# It also creates a list of document names to make it clear which score belongs to which document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-10T09:03:37.482520Z",
     "iopub.status.busy": "2021-03-10T09:03:37.481607Z",
     "iopub.status.idle": "2021-03-10T09:03:37.517567Z",
     "shell.execute_reply": "2021-03-10T09:03:37.516919Z"
    },
    "papermill": {
     "duration": 0.063763,
     "end_time": "2021-03-10T09:03:37.517700",
     "exception": false,
     "start_time": "2021-03-10T09:03:37.453937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Document1', 'Document2', 'Document3']\n",
      "[1.0, 0.7071067811865475, 0.7071067811865475]\n"
     ]
    }
   ],
   "source": [
    "def rank_similarity_docs(data):\n",
    "    cos_sim =[]\n",
    "    for doc_num in range(0 , len(data)):\n",
    "        cos_sim.append(cosine_similarity(tfidf_dict_qry, df , query , doc_num).tolist())\n",
    "    return cos_sim\n",
    "similarity_docs = rank_similarity_docs(documents)\n",
    "doc_names = [f\"Document{i+1}\" for i in range(len(documents))]\n",
    "print(doc_names)\n",
    "print(list(flatten(similarity_docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document1 has a cosine similarity score of 1.0, which indicates that \n",
    "# it's perfectly aligned with the search queryâ€”it contains the terms in the query, \n",
    "# making it the most relevant document.\n",
    "\n",
    "# Document2 and Document3 both have cosine similarity scores of approximately 0.7071, \n",
    "# suggesting that they are somewhat relevant to the search query but less so than Document1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0], [0.7071067811865475], [0.7071067811865475]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "papermill": {
   "duration": 6.967548,
   "end_time": "2021-03-10T09:03:38.716094",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-10T09:03:31.748546",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
